{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'в', 'два', 'раза', 'короче', 'и', 'в', 'пять', 'раз', 'интереснее', '2', 'почти', 'нет', 'философических', 'отступлений', '3', 'в', 'сто', 'раз', 'легче', 'читать', 'весь', 'французский', 'текст', 'заменен', 'русским', 'в', 'переводе', 'самого', 'толстого', '4', 'гораздо', 'больше', 'мира', 'и', 'меньше', 'войны', '5', 'хеппи-энд', 'эти', 'слова', 'я', 'поместил', 'семь', 'лет', 'назад', 'на', 'обложку', 'предыдущего', 'издания', 'указав', 'в', 'аннотации', 'первая', 'полная', 'редакция', 'великого', 'романа', 'созданная', 'к', 'концу', '1866', 'года', 'до', 'того', 'как', 'толстой', 'переделал', 'его', 'в', '1867--1869', 'годах', '--', 'и', 'что', 'я', 'использовал', 'такие-то', 'публикации', 'думая', 'что', 'все', 'всё', 'знают', 'я', 'не', 'объяснил', 'откуда', 'взялась', 'эта', 'первая', 'редакция', 'я', 'оказался', 'неправ', 'и', 'в', 'результате', 'оголтелые', 'и']\n"
     ]
    }
   ],
   "source": [
    "# Импортируем библиотеку для выполнения HTTP-запросов в интернет\n",
    "import requests\n",
    "from math import log\n",
    "\n",
    "# Читаем текстовый файл по url-ссылке\n",
    "data = requests.get(\"https://raw.githubusercontent.com/SkillfactoryDS/Datasets/master/war_peace_processed.txt\").text\n",
    "\n",
    "# Предобрабатываем текстовый файл\n",
    "data = data.split('\\n')\n",
    "data.remove('')\n",
    "data = data + ['[new chapter]']\n",
    "\n",
    "# Выводим первые 100 слов из книги\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяем текст на главы\n",
    "chapter_data = []\n",
    "chapter_words = []\n",
    "\n",
    "for word in data:\n",
    "    if word == '[new chapter]':\n",
    "        chapter_data.append(chapter_words)\n",
    "        chapter_words = []\n",
    "    else:\n",
    "        chapter_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Список chapter_words_count определен и содержит 171 элементов.\n"
     ]
    }
   ],
   "source": [
    "# Инициализируем список для хранения количества слов в каждой главе\n",
    "chapter_words_count = []\n",
    "\n",
    "# Считаем количество слов в каждой главе\n",
    "for chapter_words in chapter_data:\n",
    "    temp = {}\n",
    "    for word in chapter_words:\n",
    "        if word not in temp:\n",
    "            temp[word] = 1\n",
    "        else:\n",
    "            temp[word] += 1\n",
    "    chapter_words_count.append(temp)\n",
    "\n",
    "# Проверяем, что список chapter_words_count определен\n",
    "print(\"Список chapter_words_count определен и содержит\", len(chapter_words_count), \"элементов.\")\n",
    "\n",
    "# Считаем общее количество слов в каждой главе\n",
    "chapter_total_words = []\n",
    "for chapter in chapter_data:\n",
    "    chapter_total_words.append(len(chapter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007358351729212656\n"
     ]
    }
   ],
   "source": [
    "target_word = 'гостья'\n",
    "target_chapter = 15\n",
    "\n",
    "# Ваш код здесь\n",
    "\n",
    "# Считаем частоту встречаемости слов (TF) для каждой главы\n",
    "tf_per_chapter = []\n",
    "for chapter_num in range(len(chapter_words_count)):\n",
    "    chapter_dict = chapter_words_count[chapter_num]\n",
    "    chapter_total = chapter_total_words[chapter_num]\n",
    "    tf_chapter_dict = {}\n",
    "    for word in chapter_dict:\n",
    "        tf_chapter_dict[word] = chapter_dict[word] / chapter_total\n",
    "    tf_per_chapter.append(tf_chapter_dict)\n",
    "\n",
    "print(tf_per_chapter[target_chapter][target_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.672514619883041\n"
     ]
    }
   ],
   "source": [
    "target_word = 'человек'\n",
    "# Считаем частоту документов (DF) для каждого слова\n",
    "word_set = set()\n",
    "for chapter in chapter_data:\n",
    "    for word in chapter:\n",
    "        word_set.add(word)\n",
    "\n",
    "df_dict = {}\n",
    "total_chapters = len(chapter_words_count)\n",
    "for word in word_set:\n",
    "    contain_word_chapters = 0\n",
    "    for chapter_dict in chapter_words_count:\n",
    "        if word in chapter_dict:\n",
    "            contain_word_chapters += 1\n",
    "    df_dict[word] = contain_word_chapters / total_chapters\n",
    "\n",
    "print(df_dict[target_word])\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011067446769736353\n"
     ]
    }
   ],
   "source": [
    "target_word = 'анна'\n",
    "target_chapter = 4\n",
    "# Считаем нормализованное TF-IDF для каждого слова в каждой главе\n",
    "tf_idf_per_chapter = []\n",
    "for chapter_tf_dict in tf_per_chapter:\n",
    "    tf_idf_chapter_dict = {}\n",
    "    for word in chapter_tf_dict:\n",
    "        if len(word) > 3 and df_dict[word] > 0:\n",
    "            tf_idf_chapter_dict[word] = chapter_tf_dict[word] * log(1 / df_dict[word])\n",
    "    tf_idf_per_chapter.append(tf_idf_chapter_dict)\n",
    "print(tf_idf_per_chapter[target_chapter][target_word])\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем топ-3 слова с наивысшим TF-IDF в заданной главе\n",
    "target_chapter = 170  # Измените на нужный номер главы\n",
    "if target_chapter <= len(tf_idf_per_chapter):\n",
    "    chapter_tf_idf_dict = tf_idf_per_chapter[target_chapter]\n",
    "\n",
    "    # Сортируем слова по значению TF-IDF в порядке убывания\n",
    "    sorted_tf_idf = sorted(chapter_tf_idf_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Печатаем топ-3 слова\n",
    "    print(\"Топ-3 слова с наивысшим TF-IDF в главе\", target_chapter)\n",
    "    for word, tf_idf in sorted_tf_idf[:3]:\n",
    "        print(word, tf_idf)\n",
    "else:\n",
    "    print(\"Ошибка: Номер главы превышает количество глав в тексте.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
